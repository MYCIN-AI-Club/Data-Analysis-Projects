{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-03T14:48:55.322247Z","iopub.execute_input":"2022-10-03T14:48:55.322656Z","iopub.status.idle":"2022-10-03T14:48:55.338244Z","shell.execute_reply.started":"2022-10-03T14:48:55.322622Z","shell.execute_reply":"2022-10-03T14:48:55.337036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import all the Required Libraries. ","metadata":{}},{"cell_type":"markdown","source":"# **Reading Data**","metadata":{}},{"cell_type":"markdown","source":"Above code cell by default attached in notebook for preliminary setup.so the very first step to read data(train and test data both).for this purpose we use pandas library.","metadata":{}},{"cell_type":"code","source":"# Load the train and test data \n\ndf_train = pd.read_csv('../input/titanic/train.csv')\ndf_test = pd.read_csv('../input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:48:59.692325Z","iopub.execute_input":"2022-10-03T14:48:59.693025Z","iopub.status.idle":"2022-10-03T14:48:59.730736Z","shell.execute_reply.started":"2022-10-03T14:48:59.692947Z","shell.execute_reply":"2022-10-03T14:48:59.7295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get a overview about the data by viewing the first 5 rows. Below line of code will result first five row.","metadata":{}},{"cell_type":"code","source":"# first five data rows \n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:03.24174Z","iopub.execute_input":"2022-10-03T14:49:03.242171Z","iopub.status.idle":"2022-10-03T14:49:03.272539Z","shell.execute_reply.started":"2022-10-03T14:49:03.242136Z","shell.execute_reply":"2022-10-03T14:49:03.271326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"below code will give you the statistics description of data i.e. mean,medain,mode,deviation etc.","metadata":{}},{"cell_type":"code","source":"# To get a statistical description of Data \n\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T03:46:20.712431Z","iopub.execute_input":"2022-10-03T03:46:20.712802Z","iopub.status.idle":"2022-10-03T03:46:20.759538Z","shell.execute_reply.started":"2022-10-03T03:46:20.712772Z","shell.execute_reply":"2022-10-03T03:46:20.758693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get details like column name, datatype , null or not null ,memory usage\n\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:10.823008Z","iopub.execute_input":"2022-10-03T14:49:10.823876Z","iopub.status.idle":"2022-10-03T14:49:10.859364Z","shell.execute_reply.started":"2022-10-03T14:49:10.823834Z","shell.execute_reply":"2022-10-03T14:49:10.858046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Value Analysis- Training Data","metadata":{}},{"cell_type":"markdown","source":"**Count of Missing Values Before Analysis**","metadata":{}},{"cell_type":"code","source":"# Check which feature/column has how many null values\n\ndf_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:15.421411Z","iopub.execute_input":"2022-10-03T14:49:15.421837Z","iopub.status.idle":"2022-10-03T14:49:15.433834Z","shell.execute_reply.started":"2022-10-03T14:49:15.421794Z","shell.execute_reply":"2022-10-03T14:49:15.432516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# percentage missing values in cabin and age feature \n# count of missing in cabin column divided by total rows count\n\nper_age = (df_train['Age'].isnull().sum())/(df_train.shape[0])*100\nper_cabin = (df_train['Cabin'].isnull().sum())/(df_train.shape[0])*100\nprint(\"Percentage missing in Age : \",per_age)\nprint(\"Percentage Missing in Cabin : \",per_cabin)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:18.5119Z","iopub.execute_input":"2022-10-03T14:49:18.512328Z","iopub.status.idle":"2022-10-03T14:49:18.520544Z","shell.execute_reply.started":"2022-10-03T14:49:18.512295Z","shell.execute_reply":"2022-10-03T14:49:18.519127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop Missing Values or if count is more then remove that feature column Itself**","metadata":{}},{"cell_type":"code","source":"# take a deep copy of data using copy function\n# Deep copy are those in which original data is not changed if something changed in copy\n\ndf_train = df_train.copy()\n\n# Then drop Cabin Column as it has more than 77% missing values ,axis=1 means drop column not row \n\ndf_train.drop('Cabin',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:21.747766Z","iopub.execute_input":"2022-10-03T14:49:21.749051Z","iopub.status.idle":"2022-10-03T14:49:21.758416Z","shell.execute_reply.started":"2022-10-03T14:49:21.748991Z","shell.execute_reply":"2022-10-03T14:49:21.757452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get categorywise count, means whatever unique values emabarked feature have, will get count of those\n\ndf_train['Embarked'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:24.663164Z","iopub.execute_input":"2022-10-03T14:49:24.664049Z","iopub.status.idle":"2022-10-03T14:49:24.675122Z","shell.execute_reply.started":"2022-10-03T14:49:24.663996Z","shell.execute_reply":"2022-10-03T14:49:24.673877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill 'S' in place of missing values\n# Why 'S'?? - Because As per above output 'S' has too more count than 'C' and 'Q'.\n# if these are numerical data either we can put average values in place.\n# But here we have categorical, so we can't take average\n\ndf_train['Embarked'].fillna('S',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:28.182084Z","iopub.execute_input":"2022-10-03T14:49:28.182476Z","iopub.status.idle":"2022-10-03T14:49:28.188976Z","shell.execute_reply.started":"2022-10-03T14:49:28.182446Z","shell.execute_reply":"2022-10-03T14:49:28.187509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns  # library using for plots\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid') # To make a background as a grid with white color\nplt.figure(figsize=(13,6))\n\n# plot a historgram with 80 bins so that we can visualize the count for each age(from 1 to 80)\nsns.histplot(df_train['Age'],kde=False,bins=80,color='red')","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:31.287996Z","iopub.execute_input":"2022-10-03T14:49:31.28846Z","iopub.status.idle":"2022-10-03T14:49:32.466444Z","shell.execute_reply.started":"2022-10-03T14:49:31.28842Z","shell.execute_reply":"2022-10-03T14:49:32.465447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a numerical data so that either we can fill missing values with mean or median of Age col \n# Here we are filling median in place of missing values \n\ndf_train['Age'].fillna(df_train['Age'].median(),inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:36.43577Z","iopub.execute_input":"2022-10-03T14:49:36.436721Z","iopub.status.idle":"2022-10-03T14:49:36.446048Z","shell.execute_reply.started":"2022-10-03T14:49:36.436678Z","shell.execute_reply":"2022-10-03T14:49:36.444853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Count of Missing Values After Analysis**","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()\n# Count of missing values in all columns is now zero.","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:39.42149Z","iopub.execute_input":"2022-10-03T14:49:39.422252Z","iopub.status.idle":"2022-10-03T14:49:39.43337Z","shell.execute_reply.started":"2022-10-03T14:49:39.422215Z","shell.execute_reply":"2022-10-03T14:49:39.432135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create New Feature TravelAlone**\n\n* SibSp - It is number of siblings/spouses with passenger\n* Parch - It is number of parents/children with passenger\n\n* Just to reduce number of features,we can make single feature which is TravelAlone \n* Add value of SibSp and Parch column \n* If value is is more than 0 ,Put 1 in corresponding cell of TravelAlone\n* If value is not more than zero ,Put 0 in Corresponding cell of TravelAlone \n","metadata":{}},{"cell_type":"code","source":"# Create new feature TravelAlone\ndf_train['TravelAlone'] = np.where((df_train['SibSp']+df_train['Parch']) > 0, 0,1)\n\n# As we have created TravelAlone, drop both columns SibSp and Parch\ndf_train.drop('SibSp',axis=1,inplace=True)\ndf_train.drop('Parch',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:43.312239Z","iopub.execute_input":"2022-10-03T14:49:43.313076Z","iopub.status.idle":"2022-10-03T14:49:43.323151Z","shell.execute_reply.started":"2022-10-03T14:49:43.31304Z","shell.execute_reply":"2022-10-03T14:49:43.322078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dummy Encoding :** It is a concept of changing categroical variable to numeric variable. As an example ,if we have Sex column which has values 'M' for Male and 'F' for female. We can not train or process M and F characters with other numrical data.We need to convert it into numerical values.For this conversion we use Dummy Encoding Technique. \n\n* It will make Two columns for Sex feature - one is Sex_male other is Sex_female.\n* Algorithm will put 1 in sex_male column if value is 'M' in Sex column and 0 if 'F' in Sex column.\n* Vice-versa process will follow for Sex-female column as well.","metadata":{}},{"cell_type":"code","source":"# Encode 'Pclass','Sex','Embarked' features using dummy encoding technique.\n\ntrain_data = pd.get_dummies(df_train,columns=['Pclass','Sex','Embarked'])\n\n# Drop PassengerID , Name , Ticket as these features are irrelevant for training purpose.\n# There is no use for training as these are unique information of a passenger\ntrain_data.drop('PassengerId',axis=1,inplace=True)\ntrain_data.drop('Name',axis=1,inplace=True)\ntrain_data.drop('Ticket',axis=1,inplace=True)\n\n# Sex_female column we got after encoding , as sex has only two categories - Male or female\n# we can remove one of them. if 'sex_male'(which is still in training data) has value 0 it \n# means This is a female passenger ,if value the This is a Male passenger.\ntrain_data.drop('Sex_female',axis=1,inplace=True)\n\nfinal_data = train_data\nfinal_data","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:46.821987Z","iopub.execute_input":"2022-10-03T14:49:46.822417Z","iopub.status.idle":"2022-10-03T14:49:46.862738Z","shell.execute_reply.started":"2022-10-03T14:49:46.822379Z","shell.execute_reply":"2022-10-03T14:49:46.861371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:51.591885Z","iopub.execute_input":"2022-10-03T14:49:51.592285Z","iopub.status.idle":"2022-10-03T14:49:51.607349Z","shell.execute_reply.started":"2022-10-03T14:49:51.592254Z","shell.execute_reply":"2022-10-03T14:49:51.606252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = ['Age','Fare','TravelAlone','Pclass_1','Pclass_2','Pclass_3','Sex_male','Embarked_C','Embarked_Q','Embarked_S']\nX = final_data[train_features]\nY = final_data['Survived']","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:49:56.499657Z","iopub.execute_input":"2022-10-03T14:49:56.500101Z","iopub.status.idle":"2022-10-03T14:49:56.506787Z","shell.execute_reply.started":"2022-10-03T14:49:56.500064Z","shell.execute_reply":"2022-10-03T14:49:56.505621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Data Analysis of Test Data ","metadata":{}},{"cell_type":"markdown","source":"This Section Just a recap of above step thet we have performed.The only difference is that here we are going to perform analysis on Test Data and previously we preprocess the training data.Just to get a quick revision of preprocessing step on test Data.","metadata":{}},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:00.882149Z","iopub.execute_input":"2022-10-03T14:50:00.882549Z","iopub.status.idle":"2022-10-03T14:50:00.900964Z","shell.execute_reply.started":"2022-10-03T14:50:00.882518Z","shell.execute_reply":"2022-10-03T14:50:00.899807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:03.898305Z","iopub.execute_input":"2022-10-03T14:50:03.898807Z","iopub.status.idle":"2022-10-03T14:50:03.911126Z","shell.execute_reply.started":"2022-10-03T14:50:03.898767Z","shell.execute_reply":"2022-10-03T14:50:03.909869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.drop('Cabin',axis=1,inplace=True)\ndf_test['Age'].fillna(df_test['Age'].median(),inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:06.991461Z","iopub.execute_input":"2022-10-03T14:50:06.991916Z","iopub.status.idle":"2022-10-03T14:50:07.00259Z","shell.execute_reply.started":"2022-10-03T14:50:06.991879Z","shell.execute_reply":"2022-10-03T14:50:07.000041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['Fare'].fillna(df_test['Fare'].mean(),inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:10.631512Z","iopub.execute_input":"2022-10-03T14:50:10.631941Z","iopub.status.idle":"2022-10-03T14:50:10.639182Z","shell.execute_reply.started":"2022-10-03T14:50:10.631907Z","shell.execute_reply":"2022-10-03T14:50:10.637667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:14.607584Z","iopub.execute_input":"2022-10-03T14:50:14.608008Z","iopub.status.idle":"2022-10-03T14:50:14.618221Z","shell.execute_reply.started":"2022-10-03T14:50:14.607965Z","shell.execute_reply":"2022-10-03T14:50:14.616992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['TravelAlone'] = np.where((df_test['SibSp']+df_test['Parch']) > 0, 0,1)\ndf_test.drop('SibSp',axis=1,inplace=True)\ndf_test.drop('Parch',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:18.291924Z","iopub.execute_input":"2022-10-03T14:50:18.292348Z","iopub.status.idle":"2022-10-03T14:50:18.302822Z","shell.execute_reply.started":"2022-10-03T14:50:18.292316Z","shell.execute_reply":"2022-10-03T14:50:18.301531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.get_dummies(df_test,columns=['Pclass','Sex','Embarked'])\ntest_data.drop('PassengerId',axis=1,inplace=True)\ntest_data.drop('Name',axis=1,inplace=True)\ntest_data.drop('Ticket',axis=1,inplace=True)\ntest_data.drop('Sex_female',axis=1,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:21.852085Z","iopub.execute_input":"2022-10-03T14:50:21.852502Z","iopub.status.idle":"2022-10-03T14:50:21.873043Z","shell.execute_reply.started":"2022-10-03T14:50:21.852466Z","shell.execute_reply":"2022-10-03T14:50:21.87213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:25.371389Z","iopub.execute_input":"2022-10-03T14:50:25.372441Z","iopub.status.idle":"2022-10-03T14:50:25.39132Z","shell.execute_reply.started":"2022-10-03T14:50:25.372388Z","shell.execute_reply":"2022-10-03T14:50:25.39003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:29.631601Z","iopub.execute_input":"2022-10-03T14:50:29.632043Z","iopub.status.idle":"2022-10-03T14:50:30.045286Z","shell.execute_reply.started":"2022-10-03T14:50:29.632007Z","shell.execute_reply":"2022-10-03T14:50:30.044305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:32.912177Z","iopub.execute_input":"2022-10-03T14:50:32.912998Z","iopub.status.idle":"2022-10-03T14:50:32.920504Z","shell.execute_reply.started":"2022-10-03T14:50:32.912948Z","shell.execute_reply":"2022-10-03T14:50:32.919109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_log = LogisticRegression(solver='lbfgs', max_iter=1000)\nreg_log.fit(x_train,y_train)\nlog_pred = reg_log.predict(x_test)\naccuracy_score(y_test,log_pred)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:35.861628Z","iopub.execute_input":"2022-10-03T14:50:35.862499Z","iopub.status.idle":"2022-10-03T14:50:35.944436Z","shell.execute_reply.started":"2022-10-03T14:50:35.862461Z","shell.execute_reply":"2022-10-03T14:50:35.943216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_svc = LinearSVC()\nreg_svc.fit(x_train,y_train)\nsvm_pred = reg_svc.predict(x_test)\naccuracy_score(y_test,svm_pred)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:39.206374Z","iopub.execute_input":"2022-10-03T14:50:39.207141Z","iopub.status.idle":"2022-10-03T14:50:39.271581Z","shell.execute_reply.started":"2022-10-03T14:50:39.207103Z","shell.execute_reply":"2022-10-03T14:50:39.270357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_rand = RandomForestClassifier(n_estimators=100)\nreg_rand.fit(x_train,y_train)\nrf_pred = reg_rand.predict(x_test)\naccuracy_score(rf_pred,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:50:46.102439Z","iopub.execute_input":"2022-10-03T14:50:46.103087Z","iopub.status.idle":"2022-10-03T14:50:46.321826Z","shell.execute_reply.started":"2022-10-03T14:50:46.103054Z","shell.execute_reply":"2022-10-03T14:50:46.320759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_KNN = KNeighborsClassifier(n_neighbors=9,weights='distance')\nreg_KNN.fit(x_train,y_train)\nknn_pred = reg_KNN.predict(x_test)\naccuracy_score(y_test,knn_pred)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:51:30.103141Z","iopub.execute_input":"2022-10-03T14:51:30.103582Z","iopub.status.idle":"2022-10-03T14:51:30.119981Z","shell.execute_reply.started":"2022-10-03T14:51:30.103548Z","shell.execute_reply":"2022-10-03T14:51:30.119011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp = MLPClassifier(hidden_layer_sizes=(11,11,11), activation='relu', solver='adam', max_iter=500)\nmlp.fit(x_train,y_train)\n\nmlp_pred = mlp.predict(x_test)\naccuracy_score(y_test,mlp_pred)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:51:00.882261Z","iopub.execute_input":"2022-10-03T14:51:00.882672Z","iopub.status.idle":"2022-10-03T14:51:02.228565Z","shell.execute_reply.started":"2022-10-03T14:51:00.88264Z","shell.execute_reply":"2022-10-03T14:51:02.227293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_pred = reg_rand.predict(test_data)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:51:35.071288Z","iopub.execute_input":"2022-10-03T14:51:35.07223Z","iopub.status.idle":"2022-10-03T14:51:35.102143Z","shell.execute_reply.started":"2022-10-03T14:51:35.072193Z","shell.execute_reply":"2022-10-03T14:51:35.101309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({\n    \"PassengerId\" : df_test['PassengerId'],\n    \"Survived\" : final_pred\n})\noutput.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:52:38.182745Z","iopub.execute_input":"2022-10-03T14:52:38.18377Z","iopub.status.idle":"2022-10-03T14:52:38.194419Z","shell.execute_reply.started":"2022-10-03T14:52:38.18373Z","shell.execute_reply":"2022-10-03T14:52:38.19306Z"},"trusted":true},"execution_count":null,"outputs":[]}]}